{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "import xarray as xr \n",
    "import numpy as np \n",
    "import cmaps as cmap \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import warnings\n",
    "import matplotlib.patheffects as pe\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from matplotlib import rcParams\n",
    "from helper import helicity \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# rcParams['font.family'] = 'Courier New'\n",
    "\n",
    "def add_storm_dim(dataset):\n",
    "    dataset = dataset.assign_coords(case=(\"case\", dataset[\"case_number_global\"].values))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def getAngles(dataset, caseList):\n",
    "    l = []\n",
    "    for x in range(len(caseList)):\n",
    "        value = caseList[x]\n",
    "        temp = dataset.isel(num_cases = value)\n",
    "        l.append(temp.values)\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByParam(data, var, value, operator):\n",
    "    filtered = []\n",
    "    for x in range(len(data)):\n",
    "        if operator == '>' and (var[x] > value):\n",
    "            filtered.append(data[x])\n",
    "        elif operator == '>=' and (var[x] >= value):\n",
    "            filtered.append(data[x])\n",
    "        elif operator == '<' and (var[x] < value):\n",
    "            filtered.append(data[x])\n",
    "        elif operator == '<=' and (var[x] <= value):\n",
    "            filtered.append(data[x])\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defaultPlots import scatter\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "import scipy \n",
    "import requests\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def regression(input, output, RF = True, GB = False): \n",
    "    print('Input: ', input.shape, 'Output: ', output.shape)    \n",
    "    trainIn = input[:375]\n",
    "    trainOut = output[:375]\n",
    "    testIn = input[375:]\n",
    "    testOut = output[375:]\n",
    "\n",
    "    print('Training: ', trainIn.shape[0], 'Testing: ', testIn.shape[0], 'Ratio: ', trainIn.shape[0] / input.shape[0] * 100)\n",
    "\n",
    "    if RF == True:\n",
    "        regr = RandomForestRegressor(n_estimators=200, n_jobs=-1)#, max_features = 0.75, max_depth = 25)\n",
    "        regr.fit(trainIn, trainOut) \n",
    "        predictTest = regr.predict(testIn)\n",
    "    elif GB == True:\n",
    "        regr = XGBRegressor(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=10,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42\n",
    "        )\n",
    "        regr.fit(trainIn, trainOut)\n",
    "        predictTest = regr.predict(testIn)\n",
    "    else:\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(trainIn, trainOut)\n",
    "        predictTest = regr.predict(testIn)\n",
    "\n",
    "    try:\n",
    "        importance = regr.feature_importances_\n",
    "    except:\n",
    "        coef = np.abs(regr.coef_ * np.nanstd(input, axis = 0))\n",
    "        importance = coef / np.sum(coef)\n",
    "\n",
    "    corr, sig = scipy.stats.pearsonr(predictTest, testOut)\n",
    "    error2 = np.sqrt(np.mean((predictTest - testOut)**2))\n",
    "    error = np.mean(np.abs(predictTest - testOut))\n",
    "    scatter(['Predicted DVMax (kt)', predictTest], ['DVMax (kt)', testOut])#, hline = False, vline = False)\n",
    "\n",
    "    print(str(error) + f\"kt MAE\\n{str(error2)}kt RSME\\nR^2: {corr**2}\")\n",
    "\n",
    "    # import pickle\n",
    "    # with open(r\"C:\\Users\\deela\\Downloads\\SHIPS_RF_RI.cpickle\", 'wb') as f:\n",
    "    #     pickle.dump(regr, f)\n",
    "\n",
    "    return regr, predictTest, importance, testIn, testOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_mfdataset([r\"C:\\Users\\deela\\Downloads\\split\" + str(x) +\"_analysis.nc\" for x in range(1, 5)], preprocess = add_storm_dim)\n",
    "print(list(data.variables))\n",
    "\n",
    "tcradar = xr.open_mfdataset([r\"C:\\Users\\deela\\Downloads\\tc_radar_v3m_1997_2019_xy_rel_swath_ships.nc\", r\"C:\\Users\\deela\\Downloads\\tc_radar_v3m_2020_2024_xy_rel_swath_ships.nc\"], concat_dim='num_cases', combine='nested')\n",
    "sddc = tcradar[\"sddc_ships\"].sel(ships_lag_times = 0)\n",
    "shgc = tcradar['shgc_ships'].sel(ships_lag_times = 0)\n",
    "sh12 = tcradar['shgc_ships'].sel(ships_lag_times = 12)\n",
    "vent = (tcradar['shdc_ships'].sel(ships_lag_times = 0) * (100 - tcradar['rhmd_ships'].sel(ships_lag_times = 0))) / tcradar['mpi_ships'].sel(ships_lag_times = 0)\n",
    "vmax = tcradar['vmax_ships'].sel(ships_lag_times = 0)\n",
    "dtvm = vmax - tcradar['vmax_ships'].sel(ships_lag_times = -12)\n",
    "mpi = tcradar['mpi_ships'].sel(ships_lag_times = 0)\n",
    "dist = tcradar['dtl_ships'].sel(ships_lag_times = 0)\n",
    "dist12 = tcradar['dtl_ships'].sel(ships_lag_times = 12)\n",
    "ohc = tcradar['ohc_ships'].sel(ships_lag_times = 0)\n",
    "sst = tcradar['sst_ships'].sel(ships_lag_times = 0)\n",
    "sst12 = tcradar['sst_ships'].sel(ships_lag_times = 12)\n",
    "lat = tcradar['lat_ships'].sel(ships_lag_times = 0)\n",
    "rh00 = tcradar['rhmd_ships'].sel(ships_lag_times = 0)\n",
    "rh12 = tcradar['rhmd_ships'].sel(ships_lag_times = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = 360 - getAngles(sddc, data.case_number_global.values)\n",
    "shgc = getAngles(shgc, data.case_number_global.values)\n",
    "sh12 = getAngles(sh12, data.case_number_global.values)\n",
    "vent = getAngles(vent, data.case_number_global.values)\n",
    "vmax = getAngles(vmax, data.case_number_global.values)\n",
    "dtvm = getAngles(dtvm, data.case_number_global.values)\n",
    "mpi = getAngles(mpi, data.case_number_global.values)\n",
    "dist = getAngles(dist, data.case_number_global.values)\n",
    "dist12 = getAngles(dist12, data.case_number_global.values)\n",
    "ohc = getAngles(ohc, data.case_number_global.values)\n",
    "sst = getAngles(sst, data.case_number_global.values)\n",
    "sst12 = getAngles(sst12, data.case_number_global.values)\n",
    "lats = getAngles(lat, data.case_number_global.values)\n",
    "rh00 = getAngles(rh00, data.case_number_global.values)\n",
    "rhtend = rh00 - getAngles(rh12, data.case_number_global.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiltX = data['predx_mean']\n",
    "tiltY = data['predy_mean']\n",
    "dvmax = data['dvmax_24h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defaultPlots import histogram\n",
    "\n",
    "histogram(['Vortex Tilt (X)', np.array(tiltX)], bounds = [-100, 105, 5], save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xNames = ['Sea Surface Temperature', 'SST / OHC', 'VMax', 'Mid-Level RH', 'Generalized Shear (mag)', 'Deep-Layer Shear (dir)', 'VMax - MPI', 'sin(lat)', 'cos(lat)', 'RH_TEND', 'delwind_12', '12hr SHGC', 'Dist', 'Dist12', 'OHC', 'Shear * SST', 'Vent Proxy']\n",
    "x = [sst, np.array(sst) / np.array(ohc), vmax, rh00, shgc, angles, np.array(vmax) - np.array(mpi), np.sin(np.deg2rad(lats)), np.cos(np.deg2rad(lats)), rhtend, dtvm, sh12, dist, dist12, ohc, np.array(shgc) * (np.array(sst)), vent]\n",
    "y = np.array(dvmax)\n",
    "\n",
    "for i, (name, arr) in enumerate(zip(xNames, x)):\n",
    "    print(i, name, len(arr))\n",
    "\n",
    "x = [np.asarray(a, dtype=float) for a in x]\n",
    "x = [np.where(a == 9999, np.nan, a) for a in x] \n",
    "\n",
    "y = np.where(y == 9999, np.nan, y)\n",
    "\n",
    "# Start with a mask of all True\n",
    "mask = ~np.isnan(y) & ~np.isinf(y)\n",
    "\n",
    "# Update the mask based on each predictor\n",
    "for i in range(len(x)):\n",
    "    mask = mask & ~np.isnan(x[i]) & ~np.isinf(x[i])\n",
    "\n",
    "# Apply the final mask to each predictor\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i][mask]\n",
    "    # x[i] = norm(x[i])\n",
    "\n",
    "# Apply the mask to the target\n",
    "y = y[mask]\n",
    "\n",
    "# Stack predictors into 2D array and regress\n",
    "x = np.column_stack(x)\n",
    "\n",
    "regr, predictTest, importance, testIn, testOut = regression(x, y, RF = True, GB = False)\n",
    "\n",
    "feat = dict(zip(xNames, importance))\n",
    "# Print one key-value pair at a time\n",
    "for key, value in feat.items():\n",
    "    print(f\"Feature: {key:>35}, Importance: {value}\")\n",
    "print('\\n')\n",
    "# for x in range(len(storm_name)):\n",
    "#     print(storm_name[x], y[x], predictTest[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xNames = ['Sea Surface Temperature', 'SST / OHC', 'VMax', 'Mid-Level RH', 'Generalized Shear (mag)', 'Deep-Layer Shear (dir)', 'VMax - MPI', 'sin(lat)', 'cos(lat)', 'RH_TEND', 'delwind_12', '12hr SHGC', 'Dist', 'Dist12', 'OHC', 'Shear * SST', 'Vent Proxy', 'TILT X', 'TILT Y']\n",
    "x = [sst, np.array(sst) / np.array(ohc), vmax, rh00, shgc, angles, np.array(vmax) - np.array(mpi), np.sin(np.deg2rad(lats)), np.cos(np.deg2rad(lats)), rhtend, dtvm, sh12, dist, dist12, ohc, np.array(shgc) * (np.array(sst)), vent, tiltX, tiltY]\n",
    "y = np.array(dvmax)\n",
    "\n",
    "for i, (name, arr) in enumerate(zip(xNames, x)):\n",
    "    print(i, name, len(arr))\n",
    "\n",
    "x = [np.asarray(a, dtype=float) for a in x]\n",
    "x = [np.where(a == 9999, np.nan, a) for a in x] \n",
    "\n",
    "y = np.where(y == 9999, np.nan, y)\n",
    "\n",
    "# Start with a mask of all True\n",
    "mask = ~np.isnan(y) & ~np.isinf(y)\n",
    "\n",
    "# Update the mask based on each predictor\n",
    "for i in range(len(x)):\n",
    "    mask = mask & ~np.isnan(x[i]) & ~np.isinf(x[i])\n",
    "\n",
    "# Apply the final mask to each predictor\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i][mask]\n",
    "    # x[i] = norm(x[i])\n",
    "\n",
    "# Apply the mask to the target\n",
    "y = y[mask]\n",
    "\n",
    "# Stack predictors into 2D array and regress\n",
    "x = np.column_stack(x)\n",
    "\n",
    "regr, predictTest, importance, testIn, testOut = regression(x, y, RF = True, GB = False)\n",
    "\n",
    "feat = dict(zip(xNames, importance))\n",
    "# Print one key-value pair at a time\n",
    "for key, value in feat.items():\n",
    "    print(f\"Feature: {key:>35}, Importance: {value}\")\n",
    "print('\\n')\n",
    "# for x in range(len(storm_name)):\n",
    "#     print(storm_name[x], y[x], predictTest[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env5252025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
